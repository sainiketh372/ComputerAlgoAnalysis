{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainiketh372/RBL_Project/blob/main/YOLOV8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) and check software and hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "3d7b0625-1bc8-4d49-9dd0-61bb9f2f43d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 23.3/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Predict\n",
        "\n",
        "YOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLOv8 Predict Docs](https://docs.ultralytics.com/modes/train/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "127cee13-7703-4007-ab48-3c1993f3778a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
            "100% 6.23M/6.23M [00:00<00:00, 76.4MB/s]\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "\n",
            "Downloading https://ultralytics.com/images/zidane.jpg to zidane.jpg...\n",
            "100% 165k/165k [00:00<00:00, 7.59MB/s]\n",
            "image 1/1 /content/zidane.jpg: 384x640 2 persons, 1 tie, 492.5ms\n",
            "Speed: 18.5ms preprocess, 492.5ms inference, 38.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run inference on an image with YOLOv8n\n",
        "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Val\n",
        "Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. The latest YOLOv8 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLOv8 Val Docs](https://docs.ultralytics.com/modes/val/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQPtK1QYVaD_",
        "outputId": "18267fbd-0c3c-4920-8b35-427a4dea1629"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 780M/780M [00:06<00:00, 132MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download COCO val\n",
        "import torch\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58w8JLpMnjH",
        "outputId": "5484ff74-4fa2-4a60-d0f1-a741b2a45427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "\n",
            "Dataset 'coco128.yaml' images not found ⚠️, missing paths ['/content/datasets/coco128/images/train2017']\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to /content/datasets/coco128.zip...\n",
            "100% 6.66M/6.66M [00:00<00:00, 87.7MB/s]\n",
            "Unzipping /content/datasets/coco128.zip to /content/datasets...\n",
            "Dataset download success ✅ (0.6s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 17.5MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<00:00, 1531.99it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco128/labels/train2017.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 8/8 [00:41<00:00,  5.22s/it]\n",
            "                   all        128        929       0.64      0.537      0.605      0.446\n",
            "                person        128        254      0.797      0.677      0.764      0.538\n",
            "               bicycle        128          6      0.514      0.333      0.315      0.264\n",
            "                   car        128         46      0.813      0.217      0.273      0.168\n",
            "            motorcycle        128          5      0.687      0.887      0.898      0.685\n",
            "              airplane        128          6       0.82      0.833      0.927      0.675\n",
            "                   bus        128          7      0.491      0.714      0.728      0.671\n",
            "                 train        128          3      0.534      0.667      0.706      0.604\n",
            "                 truck        128         12          1      0.332      0.473      0.297\n",
            "                  boat        128          6      0.226      0.167      0.316      0.134\n",
            "         traffic light        128         14      0.734        0.2      0.202      0.139\n",
            "             stop sign        128          2          1      0.992      0.995      0.701\n",
            "                 bench        128          9      0.839      0.582       0.62      0.365\n",
            "                  bird        128         16      0.921      0.728      0.864       0.51\n",
            "                   cat        128          4      0.875          1      0.995      0.791\n",
            "                   dog        128          9      0.603      0.889      0.785      0.585\n",
            "                 horse        128          2      0.597          1      0.995      0.518\n",
            "              elephant        128         17      0.849      0.765        0.9      0.679\n",
            "                  bear        128          1      0.593          1      0.995      0.995\n",
            "                 zebra        128          4      0.848          1      0.995      0.965\n",
            "               giraffe        128          9       0.72          1      0.951      0.722\n",
            "              backpack        128          6      0.589      0.333      0.376      0.232\n",
            "              umbrella        128         18      0.804        0.5      0.643      0.414\n",
            "               handbag        128         19      0.424     0.0526      0.165     0.0889\n",
            "                   tie        128          7      0.804      0.714      0.674      0.476\n",
            "              suitcase        128          4      0.635      0.883      0.745      0.534\n",
            "               frisbee        128          5      0.675        0.8      0.759      0.688\n",
            "                  skis        128          1      0.567          1      0.995      0.497\n",
            "             snowboard        128          7      0.742      0.714      0.747        0.5\n",
            "           sports ball        128          6      0.716      0.433      0.485      0.278\n",
            "                  kite        128         10      0.817       0.45      0.569      0.184\n",
            "          baseball bat        128          4      0.551       0.25      0.353      0.175\n",
            "        baseball glove        128          7      0.624      0.429      0.429      0.293\n",
            "            skateboard        128          5      0.846        0.6        0.6       0.41\n",
            "         tennis racket        128          7      0.726      0.387      0.487       0.33\n",
            "                bottle        128         18      0.448      0.389      0.376      0.208\n",
            "            wine glass        128         16      0.743      0.362      0.584      0.333\n",
            "                   cup        128         36       0.58      0.278      0.404       0.29\n",
            "                  fork        128          6      0.527      0.167      0.246      0.184\n",
            "                 knife        128         16      0.564        0.5       0.59       0.36\n",
            "                 spoon        128         22      0.597      0.182      0.328       0.19\n",
            "                  bowl        128         28      0.648      0.643      0.618      0.491\n",
            "                banana        128          1          0          0      0.124     0.0379\n",
            "              sandwich        128          2      0.249        0.5      0.308      0.308\n",
            "                orange        128          4          1       0.31      0.995      0.623\n",
            "              broccoli        128         11      0.374      0.182      0.249      0.203\n",
            "                carrot        128         24      0.648      0.458      0.572      0.362\n",
            "               hot dog        128          2      0.351      0.553      0.745      0.721\n",
            "                 pizza        128          5      0.644          1      0.995      0.843\n",
            "                 donut        128         14      0.657          1       0.94      0.864\n",
            "                  cake        128          4      0.618          1      0.945      0.845\n",
            "                 chair        128         35      0.506      0.514      0.442      0.239\n",
            "                 couch        128          6      0.463        0.5      0.706      0.555\n",
            "          potted plant        128         14       0.65      0.643      0.711      0.472\n",
            "                   bed        128          3      0.698      0.667      0.789      0.625\n",
            "          dining table        128         13      0.432      0.615      0.485      0.366\n",
            "                toilet        128          2      0.615        0.5      0.695      0.676\n",
            "                    tv        128          2      0.373       0.62      0.745      0.696\n",
            "                laptop        128          3          1          0      0.451      0.361\n",
            "                 mouse        128          2          1          0     0.0625    0.00625\n",
            "                remote        128          8      0.843        0.5      0.605      0.529\n",
            "            cell phone        128          8          0          0     0.0549     0.0393\n",
            "             microwave        128          3      0.435      0.667      0.806      0.718\n",
            "                  oven        128          5      0.412        0.4      0.339       0.27\n",
            "                  sink        128          6       0.35      0.167      0.182      0.129\n",
            "          refrigerator        128          5      0.589        0.4      0.604      0.452\n",
            "                  book        128         29      0.629      0.103      0.346      0.178\n",
            "                 clock        128          9      0.788       0.83      0.875       0.74\n",
            "                  vase        128          2      0.376          1      0.828      0.795\n",
            "              scissors        128          1          1          0      0.249     0.0746\n",
            "            teddy bear        128         21      0.877      0.333      0.591      0.394\n",
            "            toothbrush        128          5      0.743        0.6      0.638      0.374\n",
            "Speed: 5.7ms preprocess, 302.3ms inference, 0.0ms loss, 3.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Validate YOLOv8n on COCO128 val\n",
        "!yolo val model=yolov8n.pt data=coco128.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://bit.ly/ultralytics_hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n",
        "\n",
        "Train YOLOv8 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLOv8 Train Docs](https://docs.ultralytics.com/modes/train/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "164da900-9087-464f-e7c1-3cf3c92a6cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G      1.163      1.451      1.242        294        640: 100% 8/8 [02:28<00:00, 18.58s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:42<00:00, 10.72s/it]\n",
            "                   all        128        929      0.642      0.559       0.62      0.459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G       1.21      1.432      1.262        251        640: 100% 8/8 [02:08<00:00, 16.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:41<00:00, 10.27s/it]\n",
            "                   all        128        929      0.681      0.572      0.638      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      1.103      1.312      1.199        158        640: 100% 8/8 [02:01<00:00, 15.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:47<00:00, 11.79s/it]\n",
            "                   all        128        929       0.68      0.613      0.667      0.498\n",
            "\n",
            "3 epochs completed in 0.150 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 4/4 [00:37<00:00,  9.46s/it]\n",
            "                   all        128        929       0.68      0.613      0.667      0.499\n",
            "                person        128        254       0.77      0.701       0.77      0.547\n",
            "               bicycle        128          6      0.601      0.333      0.386      0.254\n",
            "                   car        128         46      0.786      0.217      0.311      0.196\n",
            "            motorcycle        128          5      0.826      0.953      0.962       0.78\n",
            "              airplane        128          6      0.748      0.833      0.903      0.706\n",
            "                   bus        128          7      0.772      0.714      0.722       0.66\n",
            "                 train        128          3      0.459      0.667       0.83      0.753\n",
            "                 truck        128         12      0.887        0.5      0.525      0.352\n",
            "                  boat        128          6      0.567      0.333      0.627      0.419\n",
            "         traffic light        128         14      0.656      0.214      0.206       0.14\n",
            "             stop sign        128          2      0.737          1      0.995      0.697\n",
            "                 bench        128          9      0.817      0.667      0.662       0.45\n",
            "                  bird        128         16      0.778      0.875      0.964      0.622\n",
            "                   cat        128          4       0.83          1      0.995       0.76\n",
            "                   dog        128          9      0.505      0.778      0.879      0.699\n",
            "                 horse        128          2      0.708          1      0.995      0.492\n",
            "              elephant        128         17      0.824      0.941      0.948      0.777\n",
            "                  bear        128          1      0.614          1      0.995      0.995\n",
            "                 zebra        128          4      0.855          1      0.995      0.965\n",
            "               giraffe        128          9      0.774          1      0.973      0.749\n",
            "              backpack        128          6      0.492      0.325       0.37      0.208\n",
            "              umbrella        128         18      0.649       0.72      0.728       0.48\n",
            "               handbag        128         19          1      0.145      0.327      0.161\n",
            "                   tie        128          7      0.829      0.714      0.705      0.526\n",
            "              suitcase        128          4      0.581          1      0.828      0.571\n",
            "               frisbee        128          5      0.593        0.8      0.799      0.689\n",
            "                  skis        128          1      0.633          1      0.995      0.497\n",
            "             snowboard        128          7      0.647      0.714      0.777      0.547\n",
            "           sports ball        128          6      0.698        0.5      0.572      0.324\n",
            "                  kite        128         10      0.804        0.5      0.609      0.225\n",
            "          baseball bat        128          4      0.323       0.25      0.354      0.201\n",
            "        baseball glove        128          7      0.663      0.429       0.43      0.302\n",
            "            skateboard        128          5      0.496        0.6      0.603      0.401\n",
            "         tennis racket        128          7      0.588      0.412      0.467       0.32\n",
            "                bottle        128         18      0.496      0.333      0.405      0.238\n",
            "            wine glass        128         16      0.606      0.482       0.59      0.335\n",
            "                   cup        128         36      0.665      0.276      0.419      0.286\n",
            "                  fork        128          6      0.599      0.167      0.209      0.163\n",
            "                 knife        128         16      0.625      0.562      0.624      0.374\n",
            "                 spoon        128         22      0.786      0.335      0.435      0.232\n",
            "                  bowl        128         28      0.586       0.75       0.68      0.566\n",
            "                banana        128          1      0.182          1      0.199     0.0478\n",
            "              sandwich        128          2          1      0.616      0.995      0.995\n",
            "                orange        128          4          1      0.515      0.912      0.593\n",
            "              broccoli        128         11      0.509      0.273      0.287      0.236\n",
            "                carrot        128         24      0.539      0.792      0.778      0.474\n",
            "               hot dog        128          2      0.433          1      0.995      0.946\n",
            "                 pizza        128          5      0.679          1      0.995      0.866\n",
            "                 donut        128         14      0.608          1      0.919      0.837\n",
            "                  cake        128          4      0.821          1      0.995      0.904\n",
            "                 chair        128         35      0.554      0.514      0.495      0.309\n",
            "                 couch        128          6      0.555      0.424       0.61      0.455\n",
            "          potted plant        128         14      0.698      0.786      0.794      0.552\n",
            "                   bed        128          3          1      0.989      0.995      0.742\n",
            "          dining table        128         13      0.474      0.615      0.553      0.449\n",
            "                toilet        128          2      0.624        0.5      0.745      0.721\n",
            "                    tv        128          2      0.477        0.5      0.745      0.621\n",
            "                laptop        128          3          1      0.555      0.706       0.63\n",
            "                 mouse        128          2          1          0     0.0519     0.0104\n",
            "                remote        128          8      0.541        0.5      0.587      0.481\n",
            "            cell phone        128          8          1          0     0.0644     0.0316\n",
            "             microwave        128          3      0.724      0.898      0.913      0.783\n",
            "                  oven        128          5      0.517        0.4       0.41      0.306\n",
            "                  sink        128          6      0.365      0.167      0.351      0.224\n",
            "          refrigerator        128          5      0.693      0.463      0.745      0.618\n",
            "                  book        128         29      0.504      0.103      0.366      0.187\n",
            "                 clock        128          9        0.8      0.887      0.897      0.771\n",
            "                  vase        128          2       0.42          1      0.995      0.945\n",
            "              scissors        128          1          1          0      0.111      0.035\n",
            "            teddy bear        128         21      0.698      0.524      0.662      0.462\n",
            "            toothbrush        128          5          1      0.761       0.92      0.522\n",
            "Speed: 4.4ms preprocess, 266.6ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Train YOLOv8n on COCO128 for 3 epochs\n",
        "!yolo train model=yolov8n.pt data=coco128.yaml epochs=3 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZZeNrLCQG6"
      },
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- 💡 ProTip: Export to [ONNX](https://onnx.ai/) or [OpenVINO](https://docs.openvino.ai/latest/index.html) for up to 3x CPU speedup.  \n",
        "- 💡 ProTip: Export to [TensorRT](https://developer.nvidia.com/tensorrt) for up to 5x GPU speedup.\n",
        "\n",
        "\n",
        "| Format                                                                     | `format=`          | Model                     |\n",
        "|----------------------------------------------------------------------------|--------------------|---------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                            | -                  | `yolov8n.pt`              |\n",
        "| [TorchScript](https://pytorch.org/docs/stable/jit.html)                    | `torchscript`      | `yolov8n.torchscript`     |\n",
        "| [ONNX](https://onnx.ai/)                                                   | `onnx`             | `yolov8n.onnx`            |\n",
        "| [OpenVINO](https://docs.openvino.ai/latest/index.html)                     | `openvino`         | `yolov8n_openvino_model/` |\n",
        "| [TensorRT](https://developer.nvidia.com/tensorrt)                          | `engine`           | `yolov8n.engine`          |\n",
        "| [CoreML](https://github.com/apple/coremltools)                             | `coreml`           | `yolov8n.mlmodel`         |\n",
        "| [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`      | `yolov8n_saved_model/`    |\n",
        "| [TensorFlow GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`               | `yolov8n.pb`              |\n",
        "| [TensorFlow Lite](https://www.tensorflow.org/lite)                         | `tflite`           | `yolov8n.tflite`          |\n",
        "| [TensorFlow Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`          | `yolov8n_edgetpu.tflite`  |\n",
        "| [TensorFlow.js](https://www.tensorflow.org/js)                             | `tfjs`             | `yolov8n_web_model/`      |\n",
        "| [PaddlePaddle](https://github.com/PaddlePaddle)                            | `paddle`           | `yolov8n_paddle_model/`   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CYIjW4igCjqD",
        "outputId": "af9b46d3-aa53-4c2b-a1ae-bf6b16ca26c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8n.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.0.1+cu118...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 2.9s, saved as yolov8n.torchscript (12.4 MB)\n",
            "\n",
            "Export complete (3.6s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640 \n",
            "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml \n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ],
      "source": [
        "!yolo export model=yolov8n.pt format=torchscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      },
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLOv8 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLOv8 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLOv8 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bpF9-vS_DAaf",
        "outputId": "801c8090-b116-4ffe-eb75-7e81887270f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G      1.163      1.451      1.242        294        640: 100%|██████████| 8/8 [02:13<00:00, 16.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:40<00:00, 10.13s/it]\n",
            "                   all        128        929      0.642      0.559       0.62      0.459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G       1.21      1.432      1.262        251        640: 100%|██████████| 8/8 [02:09<00:00, 16.20s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:40<00:00, 10.10s/it]\n",
            "                   all        128        929      0.681      0.572      0.638      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      1.103      1.312      1.199        158        640: 100%|██████████| 8/8 [01:58<00:00, 14.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:44<00:00, 11.15s/it]\n",
            "                   all        128        929       0.68      0.613      0.667      0.498\n",
            "\n",
            "3 epochs completed in 0.144 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:36<00:00,  9.17s/it]\n",
            "                   all        128        929       0.68      0.613      0.667      0.499\n",
            "                person        128        254       0.77      0.701       0.77      0.547\n",
            "               bicycle        128          6      0.601      0.333      0.386      0.254\n",
            "                   car        128         46      0.786      0.217      0.311      0.196\n",
            "            motorcycle        128          5      0.826      0.953      0.962       0.78\n",
            "              airplane        128          6      0.748      0.833      0.903      0.706\n",
            "                   bus        128          7      0.772      0.714      0.722       0.66\n",
            "                 train        128          3      0.459      0.667       0.83      0.753\n",
            "                 truck        128         12      0.887        0.5      0.525      0.352\n",
            "                  boat        128          6      0.567      0.333      0.627      0.419\n",
            "         traffic light        128         14      0.656      0.214      0.206       0.14\n",
            "             stop sign        128          2      0.737          1      0.995      0.697\n",
            "                 bench        128          9      0.817      0.667      0.662       0.45\n",
            "                  bird        128         16      0.778      0.875      0.964      0.622\n",
            "                   cat        128          4       0.83          1      0.995       0.76\n",
            "                   dog        128          9      0.505      0.778      0.879      0.699\n",
            "                 horse        128          2      0.708          1      0.995      0.492\n",
            "              elephant        128         17      0.824      0.941      0.948      0.777\n",
            "                  bear        128          1      0.614          1      0.995      0.995\n",
            "                 zebra        128          4      0.855          1      0.995      0.965\n",
            "               giraffe        128          9      0.774          1      0.973      0.749\n",
            "              backpack        128          6      0.492      0.325       0.37      0.208\n",
            "              umbrella        128         18      0.649       0.72      0.728       0.48\n",
            "               handbag        128         19          1      0.145      0.327      0.161\n",
            "                   tie        128          7      0.829      0.714      0.705      0.526\n",
            "              suitcase        128          4      0.581          1      0.828      0.571\n",
            "               frisbee        128          5      0.593        0.8      0.799      0.689\n",
            "                  skis        128          1      0.633          1      0.995      0.497\n",
            "             snowboard        128          7      0.647      0.714      0.777      0.547\n",
            "           sports ball        128          6      0.698        0.5      0.572      0.324\n",
            "                  kite        128         10      0.804        0.5      0.609      0.225\n",
            "          baseball bat        128          4      0.323       0.25      0.354      0.201\n",
            "        baseball glove        128          7      0.663      0.429       0.43      0.302\n",
            "            skateboard        128          5      0.496        0.6      0.603      0.401\n",
            "         tennis racket        128          7      0.588      0.412      0.467       0.32\n",
            "                bottle        128         18      0.496      0.333      0.405      0.238\n",
            "            wine glass        128         16      0.606      0.482       0.59      0.335\n",
            "                   cup        128         36      0.665      0.276      0.419      0.286\n",
            "                  fork        128          6      0.599      0.167      0.209      0.163\n",
            "                 knife        128         16      0.625      0.562      0.624      0.374\n",
            "                 spoon        128         22      0.786      0.335      0.435      0.232\n",
            "                  bowl        128         28      0.586       0.75       0.68      0.566\n",
            "                banana        128          1      0.182          1      0.199     0.0478\n",
            "              sandwich        128          2          1      0.616      0.995      0.995\n",
            "                orange        128          4          1      0.515      0.912      0.593\n",
            "              broccoli        128         11      0.509      0.273      0.287      0.236\n",
            "                carrot        128         24      0.539      0.792      0.778      0.474\n",
            "               hot dog        128          2      0.433          1      0.995      0.946\n",
            "                 pizza        128          5      0.679          1      0.995      0.866\n",
            "                 donut        128         14      0.608          1      0.919      0.837\n",
            "                  cake        128          4      0.821          1      0.995      0.904\n",
            "                 chair        128         35      0.554      0.514      0.495      0.309\n",
            "                 couch        128          6      0.555      0.424       0.61      0.455\n",
            "          potted plant        128         14      0.698      0.786      0.794      0.552\n",
            "                   bed        128          3          1      0.989      0.995      0.742\n",
            "          dining table        128         13      0.474      0.615      0.553      0.449\n",
            "                toilet        128          2      0.624        0.5      0.745      0.721\n",
            "                    tv        128          2      0.477        0.5      0.745      0.621\n",
            "                laptop        128          3          1      0.555      0.706       0.63\n",
            "                 mouse        128          2          1          0     0.0519     0.0104\n",
            "                remote        128          8      0.541        0.5      0.587      0.481\n",
            "            cell phone        128          8          1          0     0.0644     0.0316\n",
            "             microwave        128          3      0.724      0.898      0.913      0.783\n",
            "                  oven        128          5      0.517        0.4       0.41      0.306\n",
            "                  sink        128          6      0.365      0.167      0.351      0.224\n",
            "          refrigerator        128          5      0.693      0.463      0.745      0.618\n",
            "                  book        128         29      0.504      0.103      0.366      0.187\n",
            "                 clock        128          9        0.8      0.887      0.897      0.771\n",
            "                  vase        128          2       0.42          1      0.995      0.945\n",
            "              scissors        128          1          1          0      0.111      0.035\n",
            "            teddy bear        128         21      0.698      0.524      0.662      0.462\n",
            "            toothbrush        128          5          1      0.761       0.92      0.522\n",
            "Speed: 3.3ms preprocess, 255.0ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:33<00:00,  4.22s/it]\n",
            "                   all        128        929      0.664      0.588      0.654      0.487\n",
            "                person        128        254      0.774      0.701      0.772      0.551\n",
            "               bicycle        128          6      0.604      0.333      0.383      0.252\n",
            "                   car        128         46       0.79      0.217      0.311      0.196\n",
            "            motorcycle        128          5      0.828       0.97      0.962       0.78\n",
            "              airplane        128          6      0.749      0.833      0.903      0.706\n",
            "                   bus        128          7      0.847      0.714      0.722       0.66\n",
            "                 train        128          3      0.461      0.667       0.83      0.753\n",
            "                 truck        128         12      0.906        0.5      0.526      0.355\n",
            "                  boat        128          6      0.487      0.333      0.523      0.367\n",
            "         traffic light        128         14      0.685      0.214      0.207       0.14\n",
            "             stop sign        128          2       0.74          1      0.995      0.697\n",
            "                 bench        128          9      0.821      0.667      0.662       0.45\n",
            "                  bird        128         16      0.809      0.875      0.964      0.622\n",
            "                   cat        128          4       0.71          1      0.995       0.76\n",
            "                   dog        128          9      0.506      0.778      0.875      0.697\n",
            "                 horse        128          2      0.711          1      0.995      0.492\n",
            "              elephant        128         17      0.826      0.941      0.948      0.777\n",
            "                  bear        128          1      0.617          1      0.995      0.995\n",
            "                 zebra        128          4      0.856          1      0.995      0.965\n",
            "               giraffe        128          9      0.769          1      0.951      0.725\n",
            "              backpack        128          6      0.488      0.321      0.354      0.198\n",
            "              umbrella        128         18      0.642      0.697      0.725      0.478\n",
            "               handbag        128         19          1      0.141      0.327      0.159\n",
            "                   tie        128          7      0.832      0.714      0.705      0.526\n",
            "              suitcase        128          4      0.614          1      0.828      0.571\n",
            "               frisbee        128          5      0.595        0.8      0.799      0.689\n",
            "                  skis        128          1      0.641          1      0.995      0.497\n",
            "             snowboard        128          7      0.651      0.714      0.771      0.544\n",
            "           sports ball        128          6      0.702        0.5      0.556      0.315\n",
            "                  kite        128         10      0.815        0.5      0.611      0.226\n",
            "          baseball bat        128          4      0.326       0.25      0.317      0.181\n",
            "        baseball glove        128          7      0.654      0.429       0.43      0.317\n",
            "            skateboard        128          5      0.792        0.6       0.61      0.415\n",
            "         tennis racket        128          7      0.744      0.419      0.502      0.338\n",
            "                bottle        128         18       0.45      0.333      0.396       0.23\n",
            "            wine glass        128         16       0.52       0.34      0.576       0.34\n",
            "                   cup        128         36      0.552      0.308      0.419      0.292\n",
            "                  fork        128          6      0.592      0.167      0.185      0.165\n",
            "                 knife        128         16      0.533      0.572      0.602      0.364\n",
            "                 spoon        128         22      0.789      0.342      0.437      0.249\n",
            "                  bowl        128         28      0.591       0.75      0.711      0.558\n",
            "                banana        128          1          0          0      0.166     0.0482\n",
            "              sandwich        128          2      0.387      0.387      0.663      0.663\n",
            "                orange        128          4          1      0.505      0.912      0.593\n",
            "              broccoli        128         11      0.451      0.273      0.282       0.23\n",
            "                carrot        128         24      0.533      0.809      0.784      0.487\n",
            "               hot dog        128          2      0.434          1      0.745      0.721\n",
            "                 pizza        128          5      0.727          1      0.995       0.85\n",
            "                 donut        128         14       0.61          1      0.911       0.83\n",
            "                  cake        128          4      0.717          1      0.995       0.88\n",
            "                 chair        128         35      0.574      0.514       0.49      0.299\n",
            "                 couch        128          6      0.627      0.569      0.719      0.538\n",
            "          potted plant        128         14        0.7      0.786      0.794      0.552\n",
            "                   bed        128          3      0.798          1      0.995      0.717\n",
            "          dining table        128         13      0.485      0.615      0.524      0.432\n",
            "                toilet        128          2      0.627        0.5      0.745      0.721\n",
            "                    tv        128          2      0.481        0.5      0.745      0.621\n",
            "                laptop        128          3          1      0.552      0.702      0.626\n",
            "                 mouse        128          2          1          0     0.0894     0.0179\n",
            "                remote        128          8      0.555        0.5      0.597      0.488\n",
            "            cell phone        128          8          1          0     0.0642     0.0316\n",
            "             microwave        128          3      0.591      0.667      0.753      0.635\n",
            "                  oven        128          5      0.519        0.4       0.41      0.306\n",
            "                  sink        128          6      0.385      0.167      0.357      0.214\n",
            "          refrigerator        128          5      0.681        0.4      0.728      0.595\n",
            "                  book        128         29      0.615      0.166      0.383      0.193\n",
            "                 clock        128          9      0.796      0.871      0.896       0.77\n",
            "                  vase        128          2      0.423          1      0.995      0.945\n",
            "              scissors        128          1          1          0      0.142     0.0444\n",
            "            teddy bear        128         21      0.701      0.524      0.659      0.459\n",
            "            toothbrush        128          5      0.687        0.4      0.826      0.508\n",
            "Speed: 2.2ms preprocess, 243.0ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/images/bus.jpg to bus.jpg...\n",
            "100%|██████████| 476k/476k [00:00<00:00, 12.2MB/s]\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 197.7ms\n",
            "Speed: 4.4ms preprocess, 197.7ms inference, 7.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/detect/train2/weights/best.pt with input shape (16, 3, 640, 640) BCHW and output shape(s) (16, 84, 8400) (6.2 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement \"onnx>=1.12.0\" not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 219.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (4.5.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx>=1.12.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 13.6s, saved as runs/detect/train2/weights/best.onnx (12.2 MB)\n",
            "\n",
            "Export complete (20.7s)\n",
            "Results saved to \u001b[1m/content/runs/detect/train2/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=runs/detect/train2/weights/best.onnx imgsz=640 \n",
            "Validate:        yolo val task=detect model=runs/detect/train2/weights/best.onnx imgsz=640 data=/usr/local/lib/python3.10/dist-packages/ultralytics/datasets/coco128.yaml \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data='coco128.yaml', epochs=3)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set\n",
        "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
        "success = model.export(format='onnx')  # export the model to ONNX format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phm9ccmOKye5"
      },
      "source": [
        "# 6. Tasks\n",
        "\n",
        "YOLOv8 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLOv8 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n",
        "\n",
        "<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/tasks/im/banner-tasks.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq26lwpYK1lq"
      },
      "source": [
        "## 1. Detection\n",
        "\n",
        "YOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8Go5qqS9LbC5",
        "outputId": "73e4a59c-2424-4b95-ee9e-ab063ede4e58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G      1.163      1.451      1.242        294        640: 100%|██████████| 8/8 [02:08<00:00, 16.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:42<00:00, 10.59s/it]\n",
            "                   all        128        929      0.642      0.559       0.62      0.459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G       1.21      1.432      1.262        251        640: 100%|██████████| 8/8 [02:07<00:00, 15.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:45<00:00, 11.34s/it]\n",
            "                   all        128        929      0.681      0.572      0.638      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      1.103      1.312      1.199        158        640: 100%|██████████| 8/8 [02:01<00:00, 15.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:48<00:00, 12.12s/it]\n",
            "                   all        128        929       0.68      0.613      0.667      0.498\n",
            "\n",
            "3 epochs completed in 0.146 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:38<00:00,  9.71s/it]\n",
            "                   all        128        929       0.68      0.613      0.667      0.499\n",
            "                person        128        254       0.77      0.701       0.77      0.547\n",
            "               bicycle        128          6      0.601      0.333      0.386      0.254\n",
            "                   car        128         46      0.786      0.217      0.311      0.196\n",
            "            motorcycle        128          5      0.826      0.953      0.962       0.78\n",
            "              airplane        128          6      0.748      0.833      0.903      0.706\n",
            "                   bus        128          7      0.772      0.714      0.722       0.66\n",
            "                 train        128          3      0.459      0.667       0.83      0.753\n",
            "                 truck        128         12      0.887        0.5      0.525      0.352\n",
            "                  boat        128          6      0.567      0.333      0.627      0.419\n",
            "         traffic light        128         14      0.656      0.214      0.206       0.14\n",
            "             stop sign        128          2      0.737          1      0.995      0.697\n",
            "                 bench        128          9      0.817      0.667      0.662       0.45\n",
            "                  bird        128         16      0.778      0.875      0.964      0.622\n",
            "                   cat        128          4       0.83          1      0.995       0.76\n",
            "                   dog        128          9      0.505      0.778      0.879      0.699\n",
            "                 horse        128          2      0.708          1      0.995      0.492\n",
            "              elephant        128         17      0.824      0.941      0.948      0.777\n",
            "                  bear        128          1      0.614          1      0.995      0.995\n",
            "                 zebra        128          4      0.855          1      0.995      0.965\n",
            "               giraffe        128          9      0.774          1      0.973      0.749\n",
            "              backpack        128          6      0.492      0.325       0.37      0.208\n",
            "              umbrella        128         18      0.649       0.72      0.728       0.48\n",
            "               handbag        128         19          1      0.145      0.327      0.161\n",
            "                   tie        128          7      0.829      0.714      0.705      0.526\n",
            "              suitcase        128          4      0.581          1      0.828      0.571\n",
            "               frisbee        128          5      0.593        0.8      0.799      0.689\n",
            "                  skis        128          1      0.633          1      0.995      0.497\n",
            "             snowboard        128          7      0.647      0.714      0.777      0.547\n",
            "           sports ball        128          6      0.698        0.5      0.572      0.324\n",
            "                  kite        128         10      0.804        0.5      0.609      0.225\n",
            "          baseball bat        128          4      0.323       0.25      0.354      0.201\n",
            "        baseball glove        128          7      0.663      0.429       0.43      0.302\n",
            "            skateboard        128          5      0.496        0.6      0.603      0.401\n",
            "         tennis racket        128          7      0.588      0.412      0.467       0.32\n",
            "                bottle        128         18      0.496      0.333      0.405      0.238\n",
            "            wine glass        128         16      0.606      0.482       0.59      0.335\n",
            "                   cup        128         36      0.665      0.276      0.419      0.286\n",
            "                  fork        128          6      0.599      0.167      0.209      0.163\n",
            "                 knife        128         16      0.625      0.562      0.624      0.374\n",
            "                 spoon        128         22      0.786      0.335      0.435      0.232\n",
            "                  bowl        128         28      0.586       0.75       0.68      0.566\n",
            "                banana        128          1      0.182          1      0.199     0.0478\n",
            "              sandwich        128          2          1      0.616      0.995      0.995\n",
            "                orange        128          4          1      0.515      0.912      0.593\n",
            "              broccoli        128         11      0.509      0.273      0.287      0.236\n",
            "                carrot        128         24      0.539      0.792      0.778      0.474\n",
            "               hot dog        128          2      0.433          1      0.995      0.946\n",
            "                 pizza        128          5      0.679          1      0.995      0.866\n",
            "                 donut        128         14      0.608          1      0.919      0.837\n",
            "                  cake        128          4      0.821          1      0.995      0.904\n",
            "                 chair        128         35      0.554      0.514      0.495      0.309\n",
            "                 couch        128          6      0.555      0.424       0.61      0.455\n",
            "          potted plant        128         14      0.698      0.786      0.794      0.552\n",
            "                   bed        128          3          1      0.989      0.995      0.742\n",
            "          dining table        128         13      0.474      0.615      0.553      0.449\n",
            "                toilet        128          2      0.624        0.5      0.745      0.721\n",
            "                    tv        128          2      0.477        0.5      0.745      0.621\n",
            "                laptop        128          3          1      0.555      0.706       0.63\n",
            "                 mouse        128          2          1          0     0.0519     0.0104\n",
            "                remote        128          8      0.541        0.5      0.587      0.481\n",
            "            cell phone        128          8          1          0     0.0644     0.0316\n",
            "             microwave        128          3      0.724      0.898      0.913      0.783\n",
            "                  oven        128          5      0.517        0.4       0.41      0.306\n",
            "                  sink        128          6      0.365      0.167      0.351      0.224\n",
            "          refrigerator        128          5      0.693      0.463      0.745      0.618\n",
            "                  book        128         29      0.504      0.103      0.366      0.187\n",
            "                 clock        128          9        0.8      0.887      0.897      0.771\n",
            "                  vase        128          2       0.42          1      0.995      0.945\n",
            "              scissors        128          1          1          0      0.111      0.035\n",
            "            teddy bear        128         21      0.698      0.524      0.662      0.462\n",
            "            toothbrush        128          5          1      0.761       0.92      0.522\n",
            "Speed: 6.0ms preprocess, 267.0ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 189.6ms\n",
            "Speed: 4.8ms preprocess, 189.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[ultralytics.yolo.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.yolo.engine.results.Boxes object\n",
              " keypoints: None\n",
              " keys: ['boxes']\n",
              " masks: None\n",
              " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: None\n",
              " speed: {'preprocess': 4.849672317504883, 'inference': 189.58210945129395, 'postprocess': 1.6567707061767578}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
        "model.train(data='coco128.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZW58jUzK66B"
      },
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "YOLOv8 _segmentation_ models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WFPJIQl_L5HT",
        "outputId": "d6f1aecf-94dc-4634-eec3-ff425a045d7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt to yolov8n-seg.pt...\n",
            "100%|██████████| 6.73M/6.73M [00:00<00:00, 79.6MB/s]\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=coco128-seg.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train\n",
            "\n",
            "Dataset 'coco128-seg.yaml' images not found ⚠️, missing paths ['/content/datasets/coco128-seg/images/train2017']\n",
            "Downloading https://ultralytics.com/assets/coco128-seg.zip to /content/datasets/coco128-seg.zip...\n",
            "100%|██████████| 6.79M/6.79M [00:00<00:00, 86.2MB/s]\n",
            "Unzipping /content/datasets/coco128-seg.zip to /content/datasets...\n",
            "Dataset download success ✅ (0.6s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1150432  ultralytics.nn.modules.head.Segment          [80, 32, 64, [64, 128, 256]]  \n",
            "YOLOv8n-seg summary: 261 layers, 3409968 parameters, 3409952 gradients\n",
            "\n",
            "Transferred 417/417 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128-seg/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<00:00, 1113.81it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco128-seg/labels/train2017.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128-seg/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/segment/train/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G      1.063      2.411      1.427      1.147        299        640: 100%|██████████| 8/8 [03:15<00:00, 24.45s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:07<00:00, 16.80s/it]\n",
            "                   all        128        929      0.602      0.549      0.588      0.439      0.569      0.519      0.547      0.362\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G      1.135      2.488        1.4       1.18        254        640: 100%|██████████| 8/8 [03:07<00:00, 23.50s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:02<00:00, 15.66s/it]\n",
            "                   all        128        929      0.592      0.592      0.618       0.46      0.581      0.563      0.581      0.377\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      1.068      2.398      1.368      1.148        165        640: 100%|██████████| 8/8 [02:56<00:00, 22.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:16<00:00, 19.05s/it]\n",
            "                   all        128        929       0.61      0.603      0.645      0.478      0.591      0.561        0.6      0.394\n",
            "\n",
            "3 epochs completed in 0.216 hours.\n",
            "Optimizer stripped from runs/segment/train/weights/last.pt, 7.1MB\n",
            "Optimizer stripped from runs/segment/train/weights/best.pt, 7.1MB\n",
            "\n",
            "Validating runs/segment/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8n-seg summary (fused): 195 layers, 3404320 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:01<00:00, 15.28s/it]\n",
            "                   all        128        929      0.607      0.604      0.645      0.478      0.591      0.561      0.599      0.394\n",
            "                person        128        254      0.794      0.699      0.785       0.55      0.775      0.651      0.734      0.399\n",
            "               bicycle        128          6      0.498      0.333      0.315      0.225      0.411      0.244      0.316      0.199\n",
            "                   car        128         46      0.534      0.217      0.311      0.182      0.558      0.217      0.266      0.114\n",
            "            motorcycle        128          5      0.663          1      0.995      0.827      0.673          1      0.995      0.591\n",
            "              airplane        128          6      0.739      0.948      0.955      0.794      0.606      0.774      0.774      0.563\n",
            "                   bus        128          7      0.653      0.714      0.729      0.628      0.657      0.714      0.721      0.592\n",
            "                 train        128          3      0.658          1      0.913      0.717      0.672          1      0.913      0.682\n",
            "                 truck        128         12        0.8      0.334       0.52      0.288      0.853      0.333       0.54      0.295\n",
            "                  boat        128          6      0.595      0.491       0.65      0.328      0.415      0.333      0.449      0.161\n",
            "         traffic light        128         14      0.767      0.214      0.229      0.145      0.525      0.143      0.156       0.14\n",
            "             stop sign        128          2          1      0.919      0.995      0.747          1       0.91      0.995      0.796\n",
            "                 bench        128          9      0.825      0.527       0.59      0.281      0.708      0.333      0.475      0.151\n",
            "                  bird        128         16      0.901      0.938      0.983      0.693      0.918      0.938      0.983      0.516\n",
            "                   cat        128          4      0.677          1      0.995      0.772      0.683          1      0.995      0.846\n",
            "                   dog        128          9      0.873      0.768      0.951      0.729      0.873      0.763      0.851       0.57\n",
            "                 horse        128          2        0.7          1      0.995      0.621      0.354        0.5      0.557      0.217\n",
            "              elephant        128         17      0.887      0.882      0.914      0.751      0.891      0.882      0.885      0.625\n",
            "                  bear        128          1      0.591          1      0.995      0.995        0.6          1      0.995      0.995\n",
            "                 zebra        128          4      0.839          1      0.995      0.972      0.842          1      0.995      0.854\n",
            "               giraffe        128          9      0.688          1      0.984      0.709      0.646      0.889      0.838      0.477\n",
            "              backpack        128          6       0.74      0.333      0.493      0.267      0.795      0.333      0.436      0.265\n",
            "              umbrella        128         18       0.49      0.722      0.669      0.459      0.422      0.611      0.426      0.221\n",
            "               handbag        128         19      0.727      0.105      0.319      0.211      0.799      0.105      0.296      0.136\n",
            "                   tie        128          7      0.748      0.571      0.621      0.437      0.786      0.571      0.621      0.378\n",
            "              suitcase        128          4      0.632       0.75       0.87      0.671      0.754       0.75       0.87      0.593\n",
            "               frisbee        128          5      0.567      0.788      0.663      0.605      0.552      0.746      0.663      0.439\n",
            "                  skis        128          1      0.594          1      0.995      0.796      0.628          1      0.995        0.2\n",
            "             snowboard        128          7      0.448      0.714      0.774      0.615      0.276      0.429      0.397      0.187\n",
            "           sports ball        128          6      0.564      0.333      0.404      0.235      0.297      0.167       0.26      0.221\n",
            "                  kite        128         10      0.529        0.3      0.408      0.209      0.538        0.3      0.331      0.157\n",
            "          baseball bat        128          4      0.482       0.25      0.406     0.0755      0.612       0.25      0.377        0.2\n",
            "        baseball glove        128          7      0.809      0.429      0.429      0.268      0.818      0.429      0.429      0.285\n",
            "            skateboard        128          5      0.537        0.2      0.304      0.158      0.551        0.2      0.332      0.165\n",
            "         tennis racket        128          7      0.511      0.429      0.433        0.3       0.53      0.429      0.433      0.281\n",
            "                bottle        128         18      0.444      0.333      0.392      0.217       0.46      0.333      0.354      0.229\n",
            "            wine glass        128         16      0.615      0.701      0.697      0.357      0.424      0.438      0.419      0.268\n",
            "                   cup        128         36      0.603      0.338      0.411      0.289      0.616      0.333      0.384       0.27\n",
            "                  fork        128          6       0.41      0.167      0.231      0.186      0.423      0.167      0.225     0.0952\n",
            "                 knife        128         16      0.718      0.479      0.589      0.402      0.673      0.438       0.51      0.359\n",
            "                 spoon        128         22      0.667      0.318      0.439       0.25      0.698      0.318      0.423      0.201\n",
            "                  bowl        128         28      0.686       0.75      0.743      0.587      0.661      0.714      0.682      0.371\n",
            "                banana        128          1     0.0905          1      0.995      0.895     0.0954          1      0.995      0.697\n",
            "              sandwich        128          2      0.327        0.5      0.448      0.428        0.4        0.5      0.448      0.388\n",
            "                orange        128          4      0.532        0.5      0.736      0.485      0.572        0.5      0.736      0.348\n",
            "              broccoli        128         11      0.334      0.182      0.288      0.242      0.345      0.182      0.318      0.246\n",
            "                carrot        128         24      0.487       0.75      0.662       0.46      0.497       0.75      0.659      0.415\n",
            "               hot dog        128          2      0.515          1      0.995      0.995      0.519          1      0.995      0.945\n",
            "                 pizza        128          5      0.573          1      0.962      0.761      0.655          1      0.962      0.765\n",
            "                 donut        128         14      0.575          1      0.943      0.883       0.58          1      0.943      0.763\n",
            "                  cake        128          4      0.684          1      0.945      0.871       0.69          1      0.945      0.821\n",
            "                 chair        128         35      0.451      0.514      0.425      0.225      0.492      0.486      0.409      0.177\n",
            "                 couch        128          6      0.608      0.781      0.678      0.525      0.461      0.576      0.514      0.312\n",
            "          potted plant        128         14      0.671      0.643      0.659      0.409      0.761      0.714      0.712      0.336\n",
            "                   bed        128          3      0.941          1      0.995      0.492      0.977          1      0.995      0.466\n",
            "          dining table        128         13      0.529      0.538      0.524      0.398      0.157      0.157      0.157     0.0807\n",
            "                toilet        128          2      0.586        0.5      0.519      0.515      0.596        0.5      0.519      0.513\n",
            "                    tv        128          2      0.454          1      0.995      0.846      0.476          1      0.995      0.896\n",
            "                laptop        128          3      0.615      0.333      0.476      0.389      0.637      0.333      0.505      0.275\n",
            "                 mouse        128          2          0          0     0.0905     0.0317          0          0     0.0273     0.0137\n",
            "                remote        128          8      0.809      0.625      0.633      0.502      0.829      0.609      0.648      0.453\n",
            "            cell phone        128          8      0.615       0.25      0.229      0.124      0.651       0.25       0.22      0.142\n",
            "             microwave        128          3      0.695      0.778      0.913      0.683      0.672      0.687      0.913      0.546\n",
            "                  oven        128          5      0.377        0.4      0.341       0.27      0.603        0.6       0.61      0.421\n",
            "                  sink        128          6       0.36      0.167       0.37       0.26       0.37      0.167       0.37      0.242\n",
            "          refrigerator        128          5       0.74      0.576      0.741      0.491      0.732      0.557      0.741      0.466\n",
            "                  book        128         29      0.331       0.12      0.319      0.152      0.321      0.103      0.218     0.0794\n",
            "                 clock        128          9      0.745      0.778      0.834      0.685      0.763      0.778      0.834      0.658\n",
            "                  vase        128          2      0.266          1      0.828      0.745      0.327          1      0.828      0.679\n",
            "              scissors        128          1          1          0          0          0          1          0          0          0\n",
            "            teddy bear        128         21      0.613      0.528      0.663      0.387      0.566      0.476      0.531      0.318\n",
            "            toothbrush        128          5      0.398        0.4      0.478      0.235      0.211        0.2      0.456      0.239\n",
            "Speed: 3.1ms preprocess, 362.1ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/train\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 1 bus, 1 skateboard, 259.9ms\n",
            "Speed: 4.3ms preprocess, 259.9ms inference, 20.5ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[ultralytics.yolo.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.yolo.engine.results.Boxes object\n",
              " keypoints: None\n",
              " keys: ['boxes', 'masks']\n",
              " masks: ultralytics.yolo.engine.results.Masks object\n",
              " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: None\n",
              " speed: {'preprocess': 4.270315170288086, 'inference': 259.9351406097412, 'postprocess': 20.496845245361328}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
        "model.train(data='coco128-seg.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax3p94VNK9zR"
      },
      "source": [
        "## 3. Classification\n",
        "\n",
        "YOLOv8 _classification_ models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5q9Zu6zlL5rS",
        "outputId": "251273b2-fdca-421e-cdae-f6fd874787e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt to yolov8n-cls.pt...\n",
            "100%|██████████| 5.28M/5.28M [00:00<00:00, 71.4MB/s]\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=mnist160, epochs=3, patience=50, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/classify/train\n",
            "\n",
            "Dataset not found ⚠️, missing path /content/datasets/mnist160, attempting download...\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/mnist160.zip to /content/datasets/mnist160.zip...\n",
            "100%|██████████| 70.0k/70.0k [00:00<00:00, 6.39MB/s]\n",
            "Unzipping /content/datasets/mnist160.zip to /content/datasets...\n",
            "Dataset download success ✅ (0.4s), saved to \u001b[1m/content/datasets/mnist160\u001b[0m\n",
            "\n",
            "Overriding model.yaml nc=1000 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    343050  ultralytics.nn.modules.head.Classify         [256, 10]                     \n",
            "YOLOv8n-cls summary: 99 layers, 1451098 parameters, 1451098 gradients\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.30000000000000004, 1.7], hue=[-0.015, 0.015]), Normalize(p=1.0, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "        1/3         0G     0.5791         16        224: 100%|██████████| 5/5 [00:04<00:00,  1.11it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s]\n",
            "                   all        0.1        0.5\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "        2/3         0G     0.5733         16        224: 100%|██████████| 5/5 [00:04<00:00,  1.00it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 3/3 [00:01<00:00,  2.56it/s]\n",
            "                   all        0.1        0.5\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "        3/3         0G     0.5738         16        224: 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]\n",
            "                   all        0.1        0.5\n",
            "\n",
            "3 epochs completed in 0.006 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 224x224 9 0.11, 8 0.11, 4 0.10, 0 0.10, 7 0.10, 24.7ms\n",
            "Speed: 1.5ms preprocess, 24.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[ultralytics.yolo.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: None\n",
              " keypoints: None\n",
              " keys: ['probs']\n",
              " masks: None\n",
              " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: ultralytics.yolo.engine.results.Probs object\n",
              " speed: {'preprocess': 1.5430450439453125, 'inference': 24.66440200805664, 'postprocess': 0.09393692016601562}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
        "model.train(data='mnist160', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpIaFLiO11TG"
      },
      "source": [
        "## 4. Pose\n",
        "\n",
        "YOLOv8 _pose_ models use the `-pose` suffix, i.e. `yolov8n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "si4aKFNg19vX",
        "outputId": "9c86cc58-7d0e-4664-8928-d4f400af43f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt to yolov8n-pose.pt...\n",
            "100%|██████████| 6.49M/6.49M [00:00<00:00, 84.2MB/s]\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=pose, mode=train, model=yolov8n-pose.pt, data=coco8-pose.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/pose/train\n",
            "\n",
            "Dataset 'coco8-pose.yaml' images not found ⚠️, missing paths ['/content/datasets/coco8-pose/images/val']\n",
            "Downloading https://ultralytics.com/assets/coco8-pose.zip to /content/datasets/coco8-pose.zip...\n",
            "100%|██████████| 334k/334k [00:00<00:00, 8.05MB/s]\n",
            "Unzipping /content/datasets/coco8-pose.zip to /content/datasets...\n",
            "Dataset download success ✅ (0.5s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1035934  ultralytics.nn.modules.head.Pose             [1, [17, 3], [64, 128, 256]]  \n",
            "YOLOv8n-pose summary: 250 layers, 3295470 parameters, 3295454 gradients\n",
            "\n",
            "Transferred 397/397 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/pose/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8-pose/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 6729.73it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8-pose/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8-pose/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 30897.27it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8-pose/labels/val.cache\n",
            "Plotting labels to runs/pose/train/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/pose/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G     0.9067      2.733     0.2828     0.7287      1.202         10        640: 100%|██████████| 1/1 [00:03<00:00,  3.58s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
            "                   all          4         14      0.926      0.895      0.907      0.668      0.846        0.5      0.535      0.345\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G      1.211      3.076     0.3337      1.182      1.245         19        640: 100%|██████████| 1/1 [00:04<00:00,  4.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n",
            "                   all          4         14      0.926      0.898      0.907      0.668      0.845        0.5      0.535      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      1.156      3.163     0.3785     0.9395      1.239         12        640: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
            "                   all          4         14      0.928      0.929      0.907      0.668      0.845        0.5      0.535      0.336\n",
            "\n",
            "3 epochs completed in 0.007 hours.\n",
            "Optimizer stripped from runs/pose/train/weights/last.pt, 6.8MB\n",
            "Optimizer stripped from runs/pose/train/weights/best.pt, 6.8MB\n",
            "\n",
            "Validating runs/pose/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.112 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8n-pose summary (fused): 187 layers, 3289964 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
            "                   all          4         14      0.926      0.894      0.907      0.668      0.846        0.5      0.535      0.345\n",
            "Speed: 2.1ms preprocess, 261.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/pose/train\u001b[0m\n",
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/bus.jpg: 640x480 4 persons, 198.5ms\n",
            "Speed: 5.1ms preprocess, 198.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[ultralytics.yolo.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.yolo.engine.results.Boxes object\n",
              " keypoints: ultralytics.yolo.engine.results.Keypoints object\n",
              " keys: ['boxes', 'keypoints']\n",
              " masks: None\n",
              " names: {0: 'person'}\n",
              " orig_img: array([[[122, 148, 172],\n",
              "         [120, 146, 170],\n",
              "         [125, 153, 177],\n",
              "         ...,\n",
              "         [157, 170, 184],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        [[127, 153, 177],\n",
              "         [124, 150, 174],\n",
              "         [127, 155, 179],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [159, 172, 186],\n",
              "         [159, 172, 186]],\n",
              " \n",
              "        [[128, 154, 178],\n",
              "         [126, 152, 176],\n",
              "         [126, 154, 178],\n",
              "         ...,\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185],\n",
              "         [158, 171, 185]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[185, 185, 191],\n",
              "         [182, 182, 188],\n",
              "         [179, 179, 185],\n",
              "         ...,\n",
              "         [114, 107, 112],\n",
              "         [115, 105, 111],\n",
              "         [116, 106, 112]],\n",
              " \n",
              "        [[157, 157, 163],\n",
              "         [180, 180, 186],\n",
              "         [185, 186, 190],\n",
              "         ...,\n",
              "         [107,  97, 103],\n",
              "         [102,  92,  98],\n",
              "         [108,  98, 104]],\n",
              " \n",
              "        [[112, 112, 118],\n",
              "         [160, 160, 166],\n",
              "         [169, 170, 174],\n",
              "         ...,\n",
              "         [ 99,  89,  95],\n",
              "         [ 96,  86,  92],\n",
              "         [102,  92,  98]]], dtype=uint8)\n",
              " orig_shape: (1080, 810)\n",
              " path: '/content/bus.jpg'\n",
              " probs: None\n",
              " speed: {'preprocess': 5.056142807006836, 'inference': 198.52137565612793, 'postprocess': 1.3723373413085938}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n classification model\n",
        "model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRKlwxSJdhd1",
        "outputId": "c0bb0484-5272-45da-c929-3ab8370b3775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 10315, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 10315 (delta 37), reused 38 (delta 14), pack-reused 10234\u001b[K\n",
            "Receiving objects: 100% (10315/10315), 6.42 MiB | 14.34 MiB/s, done.\n",
            "Resolving deltas: 100% (7029/7029), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.11, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/ultralytics, configfile: setup.cfg\n",
            "plugins: anyio-3.6.2\n",
            "collected 50 items                                                             \u001b[0m\n",
            "\n",
            "ultralytics/tests/test_cli.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m                      [ 42%]\u001b[0m\n",
            "ultralytics/tests/test_engine.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                   [ 52%]\u001b[0m\n",
            "ultralytics/tests/test_python.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                [100%]\u001b[0m\n",
            "\n",
            "============================= slowest 25 durations =============================\n",
            "45.38s call     tests/test_python.py::test_predict_img\n",
            "43.06s call     tests/test_cli.py::test_predict[detect-yolov8n-coco8.yaml]\n",
            "42.87s call     tests/test_cli.py::test_predict[classify-yolov8n-cls-imagenet10]\n",
            "41.90s call     tests/test_engine.py::test_export\n",
            "41.84s call     tests/test_cli.py::test_predict[pose-yolov8n-pose-coco8-pose.yaml]\n",
            "41.60s call     tests/test_cli.py::test_predict[segment-yolov8n-seg-coco8-seg.yaml]\n",
            "32.22s call     tests/test_cli.py::test_train[detect-yolov8n-coco8.yaml]\n",
            "31.66s call     tests/test_cli.py::test_train[pose-yolov8n-pose-coco8-pose.yaml]\n",
            "31.47s call     tests/test_python.py::test_workflow\n",
            "31.37s call     tests/test_engine.py::test_segment\n",
            "31.17s call     tests/test_cli.py::test_train[segment-yolov8n-seg-coco8-seg.yaml]\n",
            "30.15s call     tests/test_engine.py::test_detect\n",
            "20.81s call     tests/test_python.py::test_train_pretrained\n",
            "20.52s call     tests/test_python.py::test_track\n",
            "20.49s call     tests/test_python.py::test_export_openvino\n",
            "20.34s call     tests/test_cli.py::test_val[segment-yolov8n-seg-coco8-seg.yaml]\n",
            "20.23s call     tests/test_cli.py::test_special_modes\n",
            "19.70s call     tests/test_cli.py::test_val[detect-yolov8n-coco8.yaml]\n",
            "19.11s call     tests/test_cli.py::test_val[classify-yolov8n-cls-imagenet10]\n",
            "18.67s call     tests/test_python.py::test_train_scratch\n",
            "18.52s call     tests/test_cli.py::test_train[classify-yolov8n-cls-imagenet10]\n",
            "17.74s call     tests/test_python.py::test_export_onnx\n",
            "17.24s call     tests/test_cli.py::test_val[pose-yolov8n-pose-coco8-pose.yaml]\n",
            "15.47s call     tests/test_python.py::test_export_coreml\n",
            "14.56s call     tests/test_cli.py::test_export[yolov8n-seg-torchscript]\n",
            "\u001b[32m================== \u001b[32m\u001b[1m46 passed\u001b[0m, \u001b[33m4 skipped\u001b[0m\u001b[32m in 782.81s (0:13:02)\u001b[0m\u001b[32m ===================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Git clone and run tests on updates branch\n",
        "!git clone https://github.com/ultralytics/ultralytics\n",
        "!git checkout branch-99\n",
        "%pip install -qe ultralytics\n",
        "!pytest ultralytics/tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdc6t_bfzDDk",
        "outputId": "83062ac0-2213-4d4b-9b55-d19689d336d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.113 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "\n",
            "Dataset 'coco.yaml' images not found ⚠️, missing paths ['/content/datasets/coco/val2017.txt']\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip to /content/datasets/coco2017labels-segments.zip...\n",
            "100% 169M/169M [00:01<00:00, 110MB/s] \n",
            "Unzipping /content/datasets/coco2017labels-segments.zip to /content/datasets...\n",
            "Downloading http://images.cocodataset.org/zips/train2017.zip to /content/datasets/coco/images/train2017.zip...\n",
            "Downloading http://images.cocodataset.org/zips/val2017.zip to /content/datasets/coco/images/val2017.zip...\n",
            "Downloading http://images.cocodataset.org/zips/test2017.zip to /content/datasets/coco/images/test2017.zip...\n",
            "Unzipping /content/datasets/coco/images/val2017.zip to /content/datasets/coco/images...\n",
            "Unzipping /content/datasets/coco/images/test2017.zip to /content/datasets/coco/images...\n",
            "Unzipping /content/datasets/coco/images/train2017.zip to /content/datasets/coco/images...\n",
            "Dataset download success ✅ (720.6s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:15<00:00, 322.94it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/labels/val2017.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 313/313 [22:57<00:00,  4.40s/it]\n",
            "                   all       5000      36335      0.633      0.475      0.521      0.371\n",
            "                person       5000      10777      0.754      0.673      0.745      0.514\n",
            "               bicycle       5000        314      0.687      0.392      0.457      0.265\n",
            "                   car       5000       1918      0.646      0.515      0.561      0.364\n",
            "            motorcycle       5000        367       0.71       0.58      0.655      0.413\n",
            "              airplane       5000        143      0.814      0.766      0.832      0.653\n",
            "                   bus       5000        283      0.746      0.643      0.739       0.62\n",
            "                 train       5000        190      0.798       0.77      0.834      0.646\n",
            "                 truck       5000        414      0.549      0.399      0.435      0.293\n",
            "                  boat       5000        424      0.583        0.3      0.377       0.21\n",
            "         traffic light       5000        634      0.644      0.345      0.409      0.211\n",
            "          fire hydrant       5000        101       0.85      0.703      0.774      0.609\n",
            "             stop sign       5000         75      0.695       0.64      0.692       0.63\n",
            "         parking meter       5000         60      0.631        0.5      0.558      0.441\n",
            "                 bench       5000        411       0.57      0.258      0.296      0.193\n",
            "                  bird       5000        427      0.686      0.358      0.425      0.278\n",
            "                   cat       5000        202      0.776      0.824      0.856      0.652\n",
            "                   dog       5000        218      0.656      0.701      0.729      0.591\n",
            "                 horse       5000        272        0.7      0.653      0.693      0.525\n",
            "                 sheep       5000        354      0.609      0.667      0.662       0.46\n",
            "                   cow       5000        372      0.697      0.601      0.682      0.487\n",
            "              elephant       5000        252      0.702      0.833      0.821      0.629\n",
            "                  bear       5000         71      0.847      0.775      0.842      0.689\n",
            "                 zebra       5000        266      0.808      0.797      0.882      0.658\n",
            "               giraffe       5000        232      0.857      0.828      0.885      0.683\n",
            "              backpack       5000        371      0.489      0.156      0.197        0.1\n",
            "              umbrella       5000        407      0.631      0.504      0.542      0.359\n",
            "               handbag       5000        540      0.479      0.126      0.169     0.0848\n",
            "                   tie       5000        252      0.686      0.361      0.429      0.268\n",
            "              suitcase       5000        299      0.566      0.431      0.502      0.342\n",
            "               frisbee       5000        115      0.748      0.774      0.767      0.584\n",
            "                  skis       5000        241      0.609      0.323      0.366      0.188\n",
            "             snowboard       5000         69      0.483      0.319      0.388      0.266\n",
            "           sports ball       5000        260      0.713      0.438      0.474      0.329\n",
            "                  kite       5000        327      0.605       0.52      0.558       0.38\n",
            "          baseball bat       5000        145      0.585      0.386      0.403      0.216\n",
            "        baseball glove       5000        148      0.669      0.473      0.512      0.302\n",
            "            skateboard       5000        179      0.721      0.608      0.663      0.451\n",
            "             surfboard       5000        267      0.624      0.473      0.507      0.309\n",
            "         tennis racket       5000        225      0.702      0.618      0.671      0.397\n",
            "                bottle       5000       1013      0.612      0.387      0.454      0.298\n",
            "            wine glass       5000        341      0.659      0.352       0.42       0.27\n",
            "                   cup       5000        895      0.584      0.443      0.488       0.35\n",
            "                  fork       5000        215      0.583      0.318      0.389      0.263\n",
            "                 knife       5000        325      0.505      0.163      0.174      0.106\n",
            "                 spoon       5000        253       0.41      0.146       0.16     0.0988\n",
            "                  bowl       5000        623      0.589      0.494      0.529       0.39\n",
            "                banana       5000        370      0.549      0.319      0.374      0.233\n",
            "                 apple       5000        236      0.414      0.242      0.227      0.156\n",
            "              sandwich       5000        177       0.57      0.486      0.463      0.349\n",
            "                orange       5000        285      0.439      0.418      0.369      0.281\n",
            "              broccoli       5000        312      0.476       0.34      0.368      0.209\n",
            "                carrot       5000        365      0.446      0.293      0.307      0.189\n",
            "               hot dog       5000        125       0.76      0.432      0.497      0.364\n",
            "                 pizza       5000        284      0.658      0.613      0.658      0.503\n",
            "                 donut       5000        328      0.575      0.494      0.515      0.408\n",
            "                  cake       5000        310      0.533       0.39      0.437      0.292\n",
            "                 chair       5000       1771      0.581       0.34      0.403      0.257\n",
            "                 couch       5000        261      0.593      0.552      0.585      0.434\n",
            "          potted plant       5000        342      0.521      0.386      0.385      0.225\n",
            "                   bed       5000        163       0.55      0.552      0.589      0.427\n",
            "          dining table       5000        695      0.529       0.44      0.435      0.293\n",
            "                toilet       5000        179      0.721      0.737      0.775      0.642\n",
            "                    tv       5000        288      0.771      0.635      0.715      0.553\n",
            "                laptop       5000        231      0.682      0.662      0.699       0.58\n",
            "                 mouse       5000        106      0.634      0.651      0.708      0.527\n",
            "                remote       5000        283       0.43      0.223       0.27       0.16\n",
            "              keyboard       5000        153      0.595      0.604       0.65      0.482\n",
            "            cell phone       5000        262      0.548      0.351        0.4       0.28\n",
            "             microwave       5000         55      0.616      0.545      0.643      0.514\n",
            "                  oven       5000        143      0.643      0.448      0.511      0.349\n",
            "               toaster       5000          9      0.726      0.222      0.436      0.314\n",
            "                  sink       5000        225      0.568      0.444      0.508      0.337\n",
            "          refrigerator       5000        126      0.662      0.587       0.65      0.511\n",
            "                  book       5000       1129      0.489      0.111      0.197     0.0969\n",
            "                 clock       5000        267       0.75      0.607      0.667      0.457\n",
            "                  vase       5000        274      0.588      0.453      0.453      0.321\n",
            "              scissors       5000         36      0.689      0.333      0.332      0.278\n",
            "            teddy bear       5000        190      0.669      0.558      0.604       0.42\n",
            "            hair drier       5000         11          1          0    0.00492    0.00377\n",
            "            toothbrush       5000         57      0.402      0.193      0.236      0.164\n",
            "Speed: 5.0ms preprocess, 248.9ms inference, 0.0ms loss, 3.8ms postprocess per image\n",
            "Saving /content/ultralytics/runs/detect/val5/predictions.json...\n",
            "\n",
            "Evaluating pycocotools mAP using /content/ultralytics/runs/detect/val5/predictions.json and /content/datasets/coco/annotations/instances_val2017.json...\n",
            "loading annotations into memory...\n",
            "Done (t=1.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=3.99s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=105.60s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=22.61s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.373\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.525\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.405\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.185\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.320\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.532\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.588\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.366\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.768\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/val5\u001b[0m\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
            "100% 21.5M/21.5M [00:00<00:00, 38.1MB/s]\n",
            "Ultralytics YOLOv8.0.113 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 313/313 [59:35<00:00, 11.42s/it]\n",
            "                   all       5000      36335      0.682      0.563      0.613      0.447\n",
            "                person       5000      10777      0.789      0.716      0.794      0.573\n",
            "               bicycle       5000        314      0.708      0.449      0.566      0.342\n",
            "                   car       5000       1918      0.716      0.596      0.657      0.441\n",
            "            motorcycle       5000        367      0.773      0.651      0.733      0.479\n",
            "              airplane       5000        143      0.845      0.846      0.911      0.738\n",
            "                   bus       5000        283      0.813      0.754      0.818      0.701\n",
            "                 train       5000        190      0.835      0.847      0.891       0.69\n",
            "                 truck       5000        414      0.612      0.449      0.543      0.387\n",
            "                  boat       5000        424      0.637      0.409      0.475      0.266\n",
            "         traffic light       5000        634       0.73      0.451      0.524      0.275\n",
            "          fire hydrant       5000        101      0.881      0.762      0.844      0.683\n",
            "             stop sign       5000         75      0.803       0.72      0.761      0.683\n",
            "         parking meter       5000         60      0.699        0.6      0.669      0.509\n",
            "                 bench       5000        411       0.65      0.358      0.383      0.265\n",
            "                  bird       5000        427      0.671      0.454      0.518      0.347\n",
            "                   cat       5000        202      0.856      0.883      0.889      0.728\n",
            "                   dog       5000        218      0.745      0.778      0.802      0.672\n",
            "                 horse       5000        272      0.796      0.739       0.79       0.62\n",
            "                 sheep       5000        354      0.669      0.747       0.74      0.546\n",
            "                   cow       5000        372      0.751      0.694      0.747      0.564\n",
            "              elephant       5000        252      0.738      0.845       0.84      0.671\n",
            "                  bear       5000         71      0.842      0.859      0.895      0.739\n",
            "                 zebra       5000        266      0.844      0.856      0.908      0.717\n",
            "               giraffe       5000        232      0.882      0.871      0.917      0.732\n",
            "              backpack       5000        371      0.518      0.245      0.291      0.162\n",
            "              umbrella       5000        407      0.688       0.59      0.645      0.443\n",
            "               handbag       5000        540      0.547      0.235      0.293      0.161\n",
            "                   tie       5000        252      0.712      0.472      0.533      0.343\n",
            "              suitcase       5000        299       0.64      0.569      0.643      0.445\n",
            "               frisbee       5000        115      0.836       0.84      0.883      0.696\n",
            "                  skis       5000        241      0.667      0.407      0.478      0.258\n",
            "             snowboard       5000         69      0.614      0.464      0.531      0.393\n",
            "           sports ball       5000        260       0.74      0.535      0.582      0.422\n",
            "                  kite       5000        327      0.648      0.596      0.642      0.461\n",
            "          baseball bat       5000        145      0.624      0.539      0.551      0.336\n",
            "        baseball glove       5000        148       0.74      0.588      0.626      0.385\n",
            "            skateboard       5000        179      0.808      0.743      0.782      0.573\n",
            "             surfboard       5000        267      0.771      0.566       0.62      0.411\n",
            "         tennis racket       5000        225      0.796      0.747      0.787      0.537\n",
            "                bottle       5000       1013      0.657      0.506      0.565      0.387\n",
            "            wine glass       5000        341      0.709      0.475      0.572      0.371\n",
            "                   cup       5000        895      0.648      0.552      0.597      0.437\n",
            "                  fork       5000        215      0.677      0.459      0.561      0.387\n",
            "                 knife       5000        325      0.514      0.258      0.315      0.196\n",
            "                 spoon       5000        253       0.49      0.277       0.29      0.188\n",
            "                  bowl       5000        623      0.631      0.579      0.608      0.455\n",
            "                banana       5000        370      0.552      0.341       0.41      0.267\n",
            "                 apple       5000        236      0.442      0.272      0.248       0.18\n",
            "              sandwich       5000        177      0.535      0.486      0.528      0.398\n",
            "                orange       5000        285      0.487      0.372      0.381      0.294\n",
            "              broccoli       5000        312      0.502      0.365      0.389       0.23\n",
            "                carrot       5000        365      0.459      0.373      0.353      0.228\n",
            "               hot dog       5000        125      0.668      0.432      0.518      0.411\n",
            "                 pizza       5000        284      0.743       0.69      0.731      0.564\n",
            "                 donut       5000        328      0.597      0.552      0.564      0.449\n",
            "                  cake       5000        310      0.628      0.535      0.564      0.391\n",
            "                 chair       5000       1771      0.651      0.425      0.507      0.337\n",
            "                 couch       5000        261      0.639      0.613      0.656      0.499\n",
            "          potted plant       5000        342      0.537      0.427      0.463      0.286\n",
            "                   bed       5000        163      0.607       0.62      0.647      0.463\n",
            "          dining table       5000        695       0.57      0.475      0.484      0.335\n",
            "                toilet       5000        179      0.773      0.777      0.837      0.677\n",
            "                    tv       5000        288      0.725      0.708      0.777      0.606\n",
            "                laptop       5000        231      0.744      0.753      0.775      0.649\n",
            "                 mouse       5000        106      0.767      0.726       0.79      0.613\n",
            "                remote       5000        283      0.624      0.413      0.497      0.309\n",
            "              keyboard       5000        153       0.67      0.649       0.73      0.556\n",
            "            cell phone       5000        262      0.589      0.481      0.531       0.37\n",
            "             microwave       5000         55      0.685      0.712      0.767      0.617\n",
            "                  oven       5000        143      0.639      0.558      0.606      0.412\n",
            "               toaster       5000          9      0.621      0.731       0.82      0.608\n",
            "                  sink       5000        225      0.625      0.538      0.606      0.398\n",
            "          refrigerator       5000        126      0.794      0.673      0.775      0.644\n",
            "                  book       5000       1129      0.536      0.162      0.257      0.137\n",
            "                 clock       5000        267      0.713       0.67      0.714      0.503\n",
            "                  vase       5000        274       0.61      0.536      0.542      0.388\n",
            "              scissors       5000         36      0.671      0.389      0.434       0.35\n",
            "            teddy bear       5000        190      0.741      0.632      0.676      0.515\n",
            "            hair drier       5000         11          1          0      0.114     0.0511\n",
            "            toothbrush       5000         57      0.472      0.368      0.343       0.24\n",
            "Speed: 4.7ms preprocess, 688.7ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
            "Saving /content/ultralytics/runs/detect/val6/predictions.json...\n",
            "\n",
            "Evaluating pycocotools mAP using /content/ultralytics/runs/detect/val6/predictions.json and /content/datasets/coco/annotations/instances_val2017.json...\n",
            "loading annotations into memory...\n",
            "Done (t=1.61s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=6.62s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=101.27s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=19.08s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.618\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.486\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.257\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.499\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.610\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.355\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.590\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.643\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.435\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.708\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.800\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/val6\u001b[0m\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt to yolov8m.pt...\n",
            "100% 49.7M/49.7M [00:00<00:00, 72.5MB/s]\n",
            "Ultralytics YOLOv8.0.113 🚀 Python-3.10.11 torch-2.0.1+cu118 CPU\n",
            "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  16% 50/313 [18:48<1:41:42, 23.20s/it]"
          ]
        }
      ],
      "source": [
        "# Validate multiple models\n",
        "for x in 'nsmlx':\n",
        "  !yolo val model=yolov8{x}.pt data=coco.yaml"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1--Z5g1OQiedaJY3Lnbo83tw7pOEs_PxQ",
      "authorship_tag": "ABX9TyPPGX1NjE5MmXcBTpALuWv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}